{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import related packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchtuples as tt\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from pycox.models.utils import pad_col\n",
    "from pycox.preprocessing.label_transforms import LabTransDiscreteTime\n",
    "from pycox import models\n",
    "from typing import Tuple\n",
    "from torch import Tensor\n",
    "from pycox.models import utils\n",
    "from torchtuples import TupleTree\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils import shuffle\n",
    "from itertools import product\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/')\n",
    "from eval import EvalSurv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretization of survival times\n",
    "class LabTransform(LabTransDiscreteTime):\n",
    "    def transform(self, durations, events):\n",
    "        durations, is_event = super().transform(durations, events > 0)\n",
    "        events[is_event == 0] = 0\n",
    "        return durations, events.astype('int64')\n",
    "\n",
    "def index_3d_to_1d(i, j, k, n):\n",
    "    \"\"\"\n",
    "    Converts a 3D index (i, j, k) from sets T1, T2, and cancertype, each with elements ranging from 1 to n, to a 1D index.\n",
    "    \"\"\"\n",
    "    return i * n * n + j * n + k\n",
    "\n",
    "def index_2d_to_1d(i, j, n):\n",
    "    \"\"\"\n",
    "    Converts a 2D index (i, j) from sets T1 and T2, each with elements ranging from 1 to n, to a 1D index.\n",
    "    \"\"\"\n",
    "    return i * n + j\n",
    "\n",
    "# Neural network architecture\n",
    "class MaskedLinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True, mask=None):\n",
    "        super(MaskedLinearLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if bias:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "        self.mask = mask if mask is not None else torch.ones(out_features, in_features)\n",
    "\n",
    "    def forward(self, input):\n",
    "        masked_weight = self.weight * self.mask\n",
    "        return nn.functional.linear(input, masked_weight, self.bias)\n",
    "\n",
    "class pretrain_survival(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, num_nodes_shared, num_nodes_indiv, num_T1,\n",
    "                 out_features, batch_norm=True, dropout=None, mask=None):\n",
    "        super().__init__()\n",
    "        self.first_layer = MaskedLinearLayer(in_features, num_nodes_shared[0], mask=mask)\n",
    "        self.shared_net = tt.practical.MLPVanilla(\n",
    "            num_nodes_shared[0], num_nodes_shared[1:-1], num_nodes_shared[-1],\n",
    "            batch_norm, dropout,\n",
    "        )\n",
    "        self.risk_nets = torch.nn.ModuleList()\n",
    "        for _ in range(32 * num_T1):\n",
    "            net = tt.practical.MLPVanilla(\n",
    "                num_nodes_shared[-1], num_nodes_indiv, out_features,\n",
    "                batch_norm, dropout,\n",
    "            )\n",
    "            self.risk_nets.append(net)\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = self.first_layer(input)\n",
    "        out = self.shared_net(out)\n",
    "        out_list = [net(out) for net in self.risk_nets]\n",
    "        out = torch.stack(out_list, dim=1).view(out.size(0), 32, num_T1, out_features)\n",
    "        return out\n",
    "\n",
    "class finetune_survival(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, num_nodes_shared, num_nodes_indiv, num_T1,\n",
    "                 out_features, batch_norm=True, dropout=None, mask=None):\n",
    "        super().__init__()\n",
    "        self.first_layer = MaskedLinearLayer(in_features, num_nodes_shared[0], mask=mask)\n",
    "        self.shared_net = tt.practical.MLPVanilla(\n",
    "            num_nodes_shared[0], num_nodes_shared[1:-1], num_nodes_shared[-1],\n",
    "            batch_norm, dropout,\n",
    "        )\n",
    "        self.risk_nets = torch.nn.ModuleList()\n",
    "        for _ in range(32 * num_T1): \n",
    "            net = tt.practical.MLPVanilla(\n",
    "                num_nodes_shared[-1], num_nodes_indiv, out_features,\n",
    "                batch_norm, dropout,\n",
    "            )\n",
    "            self.risk_nets.append(net)\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = self.first_layer(input)\n",
    "        out = self.shared_net(out)\n",
    "        out_list = [net(out) for net in self.risk_nets]\n",
    "        out = torch.stack(out_list, dim=1).view(out.size(0), 32, num_T1, out_features)\n",
    "        return out\n",
    "\n",
    "class multi_task(tt.Model):\n",
    "\n",
    "    def __init__(self, net, optimizer=None, device=None, alpha=0.2, sigma=0.1, duration_index=None, loss=None):\n",
    "        self.duration_index = duration_index\n",
    "        if loss is None:\n",
    "            loss = Loss1(alpha, sigma)\n",
    "        super().__init__(net, loss, optimizer, device)\n",
    "\n",
    "    @property\n",
    "    def duration_index(self):\n",
    "        \n",
    "        return self._duration_index\n",
    "\n",
    "    @duration_index.setter\n",
    "    def duration_index(self, val):\n",
    "        self._duration_index = val\n",
    "\n",
    "    def make_dataloader(self, data, batch_size, shuffle, num_workers=0):\n",
    "        dataloader = super().make_dataloader(data, batch_size, shuffle, num_workers,\n",
    "                                             make_dataset=models.data.DeepHitDataset)\n",
    "        return dataloader\n",
    "    \n",
    "    def make_dataloader_predict(self, input, batch_size, shuffle=False, num_workers=0):\n",
    "        dataloader = super().make_dataloader(input, batch_size, shuffle, num_workers)\n",
    "        return dataloader\n",
    "\n",
    "    def predict_pmf_12(self, input, batch_size=8224, numpy=None, eval_=True,\n",
    "                     to_cpu=False, num_workers=0):\n",
    " \n",
    "        preds = self.predict(input, batch_size, False, eval_, False, to_cpu, num_workers)\n",
    "        pmf = pad_col(preds.view(preds.size(0), -1)).softmax(1)[:, :-1]\n",
    "        pmf = pmf.view(preds.shape).transpose(0, 1).transpose(1, 2).transpose(2, 3)\n",
    "        return tt.utils.array_or_tensor(pmf, numpy, input)\n",
    "\n",
    "def _reduction(loss: Tensor, reduction: str = 'mean') -> Tensor:\n",
    "    if reduction == 'none':\n",
    "        return loss\n",
    "    elif reduction == 'mean':\n",
    "        return loss.mean()\n",
    "    elif reduction == 'sum':\n",
    "        return loss.sum()\n",
    "    raise ValueError(f\"`reduction` = {reduction} is not valid. Use 'none', 'mean' or 'sum'.\")\n",
    "\n",
    "def index_3d_from_1d(k, n):\n",
    "    \"\"\"\n",
    "    Converts a 1D index k to a 3D index (i, j, m) assuming each dimension ranges from 0 to n-1.\n",
    "    \"\"\"\n",
    "    i = k // (n * n)\n",
    "    k = k % (n * n)\n",
    "    j = k // n\n",
    "    m = k % n\n",
    "    return (i, j, m)\n",
    "\n",
    "# Parameter optimization and loss function\n",
    "def nll_pmf_cr(phi: Tensor, idx_durations: Tensor, events: Tensor, reduction: str = 'mean',\n",
    "               epsilon: float = 1e-7) -> Tensor:\n",
    "\n",
    "    events = events.view(-1)\n",
    "    event_00 = (events == 0).float()\n",
    "    event_01 = (events == 1).float()\n",
    "    event_02 = (events == 2).float()\n",
    "    event_03 = (events == 3).float()\n",
    "    \n",
    "    idx_durations1, idx_durations2, idx_durations3 = index_3d_from_1d(idx_durations.view(-1), 32)\n",
    "    batch_size = phi.size(0)\n",
    "    sm = utils.pad_col(phi.view(batch_size, -1)).softmax(1)[:, :-1].view(phi.shape)\n",
    "    index = torch.arange(batch_size)\n",
    "    part1 = sm[index, idx_durations3, idx_durations1, idx_durations2].relu().add(epsilon).log().mul(event_03)\n",
    "    part2 = (sm[index, idx_durations3, idx_durations1, :].sum(1) - sm.cumsum(3)[index, idx_durations3, idx_durations1, idx_durations2]).relu().add(epsilon).log().mul(event_02)\n",
    "    part3 = (sm[index, idx_durations3, :, idx_durations2].sum(1) - sm.cumsum(2)[index, idx_durations3, idx_durations1, idx_durations2]).relu().add(epsilon).log().mul(event_01)\n",
    "    part4 = (1 - sm.cumsum(3)[index, idx_durations3, :, idx_durations2].sum(1) + sm.cumsum(2).cumsum(3)[index, idx_durations3, idx_durations1, idx_durations2] - sm.cumsum(2)[index, idx_durations3, idx_durations1, :].sum(1)).relu().add(epsilon).log().mul(event_00)    \n",
    "\n",
    "    loss = - part1.add(part2).add(part3).add(part4)\n",
    "    return _reduction(loss, reduction)\n",
    "\n",
    "class _Loss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, reduction: str = 'mean') -> None:\n",
    "        super().__init__()\n",
    "        self.reduction = reduction\n",
    "\n",
    "class _Loss1(_Loss):\n",
    "\n",
    "    def __init__(self, alpha: float, sigma: float, reduction: str = 'mean') -> None:\n",
    "        super().__init__(reduction)\n",
    "        self.alpha = alpha\n",
    "        self.sigma = sigma\n",
    "\n",
    "    @property\n",
    "    def alpha(self) -> float:\n",
    "        return self._alpha\n",
    "\n",
    "    @alpha.setter\n",
    "    def alpha(self, alpha: float) -> None:\n",
    "        if (alpha < 0) or (alpha > 1):\n",
    "            raise ValueError(f\"Need `alpha` to be in [0, 1]. Got {alpha}.\")\n",
    "        self._alpha = alpha\n",
    "\n",
    "    @property\n",
    "    def sigma(self) -> float:\n",
    "        return self._sigma\n",
    "\n",
    "    @sigma.setter\n",
    "    def sigma(self, sigma: float) -> None:\n",
    "        if sigma <= 0:\n",
    "            raise ValueError(f\"Need `sigma` to be positive. Got {sigma}.\")\n",
    "        self._sigma = sigma\n",
    "\n",
    "class Loss1(_Loss1):\n",
    "\n",
    "    def forward(self, phi: Tensor, idx_durations: Tensor, events: Tensor, rank_mat: Tensor) -> Tensor:\n",
    "        nll =  nll_pmf_cr(phi, idx_durations, events, self.reduction)\n",
    "        return nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed to make this reproducable\n",
    "np.random.seed(123)\n",
    "_ = torch.manual_seed(123)\n",
    "\n",
    "# Import data\n",
    "relevantdata = pd.read_csv('relevantdata.txt', header=0)\n",
    "targetdata = pd.read_csv('targetdata.txt', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total event1 rate: 0.6674107142857143, Total event2 rate: 0.46875\n",
      "Train event1 rate: 0.6675977653631285, Train event2 rate: 0.4692737430167598\n",
      "Test event1 rate: 0.6666666666666666, Test event2 rate: 0.4666666666666667\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ABCA1</th>\n",
       "      <th>ABCA10</th>\n",
       "      <th>ABCA12</th>\n",
       "      <th>ABCA13</th>\n",
       "      <th>ABCA2</th>\n",
       "      <th>ABCA3</th>\n",
       "      <th>ABCA5</th>\n",
       "      <th>ABCA6</th>\n",
       "      <th>ABCA7</th>\n",
       "      <th>ABCA8</th>\n",
       "      <th>...</th>\n",
       "      <th>SFRP5</th>\n",
       "      <th>TBL1X</th>\n",
       "      <th>TBL1XR1</th>\n",
       "      <th>VANGL1</th>\n",
       "      <th>VANGL2</th>\n",
       "      <th>event2</th>\n",
       "      <th>T2</th>\n",
       "      <th>event1</th>\n",
       "      <th>T1</th>\n",
       "      <th>cancertype_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.38</td>\n",
       "      <td>2.66</td>\n",
       "      <td>1.67</td>\n",
       "      <td>7.03</td>\n",
       "      <td>7.52</td>\n",
       "      <td>6.14</td>\n",
       "      <td>6.69</td>\n",
       "      <td>5.18</td>\n",
       "      <td>8.21</td>\n",
       "      <td>9.49</td>\n",
       "      <td>...</td>\n",
       "      <td>0.70</td>\n",
       "      <td>6.39</td>\n",
       "      <td>10.85</td>\n",
       "      <td>10.18</td>\n",
       "      <td>9.66</td>\n",
       "      <td>1</td>\n",
       "      <td>1766</td>\n",
       "      <td>1</td>\n",
       "      <td>1441</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.65</td>\n",
       "      <td>2.04</td>\n",
       "      <td>11.32</td>\n",
       "      <td>4.00</td>\n",
       "      <td>12.06</td>\n",
       "      <td>9.74</td>\n",
       "      <td>7.96</td>\n",
       "      <td>6.31</td>\n",
       "      <td>9.97</td>\n",
       "      <td>8.34</td>\n",
       "      <td>...</td>\n",
       "      <td>5.44</td>\n",
       "      <td>9.96</td>\n",
       "      <td>9.34</td>\n",
       "      <td>9.92</td>\n",
       "      <td>10.68</td>\n",
       "      <td>0</td>\n",
       "      <td>1160</td>\n",
       "      <td>0</td>\n",
       "      <td>1160</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.91</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.24</td>\n",
       "      <td>10.96</td>\n",
       "      <td>10.57</td>\n",
       "      <td>7.73</td>\n",
       "      <td>0.66</td>\n",
       "      <td>9.95</td>\n",
       "      <td>1.73</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.69</td>\n",
       "      <td>10.43</td>\n",
       "      <td>10.00</td>\n",
       "      <td>8.15</td>\n",
       "      <td>0</td>\n",
       "      <td>490</td>\n",
       "      <td>0</td>\n",
       "      <td>490</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.06</td>\n",
       "      <td>2.71</td>\n",
       "      <td>7.97</td>\n",
       "      <td>3.01</td>\n",
       "      <td>10.76</td>\n",
       "      <td>9.63</td>\n",
       "      <td>6.59</td>\n",
       "      <td>4.80</td>\n",
       "      <td>9.67</td>\n",
       "      <td>7.25</td>\n",
       "      <td>...</td>\n",
       "      <td>5.87</td>\n",
       "      <td>9.26</td>\n",
       "      <td>9.09</td>\n",
       "      <td>10.13</td>\n",
       "      <td>10.67</td>\n",
       "      <td>1</td>\n",
       "      <td>472</td>\n",
       "      <td>0</td>\n",
       "      <td>472</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.10</td>\n",
       "      <td>3.94</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.44</td>\n",
       "      <td>10.67</td>\n",
       "      <td>10.82</td>\n",
       "      <td>9.59</td>\n",
       "      <td>7.78</td>\n",
       "      <td>8.87</td>\n",
       "      <td>13.13</td>\n",
       "      <td>...</td>\n",
       "      <td>0.73</td>\n",
       "      <td>9.31</td>\n",
       "      <td>11.02</td>\n",
       "      <td>10.18</td>\n",
       "      <td>5.21</td>\n",
       "      <td>0</td>\n",
       "      <td>2249</td>\n",
       "      <td>0</td>\n",
       "      <td>2249</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>7.63</td>\n",
       "      <td>2.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.75</td>\n",
       "      <td>9.42</td>\n",
       "      <td>5.68</td>\n",
       "      <td>7.30</td>\n",
       "      <td>3.63</td>\n",
       "      <td>6.24</td>\n",
       "      <td>4.78</td>\n",
       "      <td>...</td>\n",
       "      <td>1.11</td>\n",
       "      <td>9.55</td>\n",
       "      <td>11.64</td>\n",
       "      <td>7.82</td>\n",
       "      <td>9.54</td>\n",
       "      <td>0</td>\n",
       "      <td>3932</td>\n",
       "      <td>0</td>\n",
       "      <td>3932</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>9.88</td>\n",
       "      <td>3.04</td>\n",
       "      <td>6.62</td>\n",
       "      <td>3.37</td>\n",
       "      <td>11.89</td>\n",
       "      <td>8.92</td>\n",
       "      <td>9.11</td>\n",
       "      <td>4.08</td>\n",
       "      <td>8.51</td>\n",
       "      <td>4.36</td>\n",
       "      <td>...</td>\n",
       "      <td>3.16</td>\n",
       "      <td>9.05</td>\n",
       "      <td>10.56</td>\n",
       "      <td>9.87</td>\n",
       "      <td>10.40</td>\n",
       "      <td>0</td>\n",
       "      <td>405</td>\n",
       "      <td>0</td>\n",
       "      <td>405</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>13.10</td>\n",
       "      <td>1.89</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.11</td>\n",
       "      <td>10.07</td>\n",
       "      <td>11.60</td>\n",
       "      <td>8.52</td>\n",
       "      <td>3.27</td>\n",
       "      <td>8.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.43</td>\n",
       "      <td>10.73</td>\n",
       "      <td>10.33</td>\n",
       "      <td>8.84</td>\n",
       "      <td>1</td>\n",
       "      <td>468</td>\n",
       "      <td>1</td>\n",
       "      <td>335</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>10.29</td>\n",
       "      <td>7.52</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.85</td>\n",
       "      <td>11.58</td>\n",
       "      <td>9.13</td>\n",
       "      <td>9.23</td>\n",
       "      <td>11.74</td>\n",
       "      <td>10.16</td>\n",
       "      <td>11.03</td>\n",
       "      <td>...</td>\n",
       "      <td>1.93</td>\n",
       "      <td>8.71</td>\n",
       "      <td>10.94</td>\n",
       "      <td>8.69</td>\n",
       "      <td>9.03</td>\n",
       "      <td>0</td>\n",
       "      <td>3176</td>\n",
       "      <td>0</td>\n",
       "      <td>3176</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>11.20</td>\n",
       "      <td>3.33</td>\n",
       "      <td>3.21</td>\n",
       "      <td>3.78</td>\n",
       "      <td>9.30</td>\n",
       "      <td>8.25</td>\n",
       "      <td>9.31</td>\n",
       "      <td>6.78</td>\n",
       "      <td>9.09</td>\n",
       "      <td>4.78</td>\n",
       "      <td>...</td>\n",
       "      <td>0.50</td>\n",
       "      <td>9.33</td>\n",
       "      <td>10.96</td>\n",
       "      <td>9.92</td>\n",
       "      <td>8.94</td>\n",
       "      <td>1</td>\n",
       "      <td>4507</td>\n",
       "      <td>1</td>\n",
       "      <td>4126</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>358 rows × 4033 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ABCA1  ABCA10  ABCA12  ABCA13  ABCA2  ABCA3  ABCA5  ABCA6  ABCA7  ABCA8  \\\n",
       "0     7.38    2.66    1.67    7.03   7.52   6.14   6.69   5.18   8.21   9.49   \n",
       "1     8.65    2.04   11.32    4.00  12.06   9.74   7.96   6.31   9.97   8.34   \n",
       "2     8.91    1.11    0.00    4.24  10.96  10.57   7.73   0.66   9.95   1.73   \n",
       "3     9.06    2.71    7.97    3.01  10.76   9.63   6.59   4.80   9.67   7.25   \n",
       "4    10.10    3.94    0.00    8.44  10.67  10.82   9.59   7.78   8.87  13.13   \n",
       "..     ...     ...     ...     ...    ...    ...    ...    ...    ...    ...   \n",
       "353   7.63    2.42    0.00    4.75   9.42   5.68   7.30   3.63   6.24   4.78   \n",
       "354   9.88    3.04    6.62    3.37  11.89   8.92   9.11   4.08   8.51   4.36   \n",
       "355  13.10    1.89    0.00    5.11  10.07  11.60   8.52   3.27   8.85   0.00   \n",
       "356  10.29    7.52    0.49    0.85  11.58   9.13   9.23  11.74  10.16  11.03   \n",
       "357  11.20    3.33    3.21    3.78   9.30   8.25   9.31   6.78   9.09   4.78   \n",
       "\n",
       "     ...  SFRP5  TBL1X  TBL1XR1  VANGL1  VANGL2  event2    T2  event1    T1  \\\n",
       "0    ...   0.70   6.39    10.85   10.18    9.66       1  1766       1  1441   \n",
       "1    ...   5.44   9.96     9.34    9.92   10.68       0  1160       0  1160   \n",
       "2    ...   0.00   9.69    10.43   10.00    8.15       0   490       0   490   \n",
       "3    ...   5.87   9.26     9.09   10.13   10.67       1   472       0   472   \n",
       "4    ...   0.73   9.31    11.02   10.18    5.21       0  2249       0  2249   \n",
       "..   ...    ...    ...      ...     ...     ...     ...   ...     ...   ...   \n",
       "353  ...   1.11   9.55    11.64    7.82    9.54       0  3932       0  3932   \n",
       "354  ...   3.16   9.05    10.56    9.87   10.40       0   405       0   405   \n",
       "355  ...   0.00   9.43    10.73   10.33    8.84       1   468       1   335   \n",
       "356  ...   1.93   8.71    10.94    8.69    9.03       0  3176       0  3176   \n",
       "357  ...   0.50   9.33    10.96    9.92    8.94       1  4507       1  4126   \n",
       "\n",
       "     cancertype_num  \n",
       "0                31  \n",
       "1                31  \n",
       "2                31  \n",
       "3                31  \n",
       "4                31  \n",
       "..              ...  \n",
       "353              31  \n",
       "354              31  \n",
       "355              31  \n",
       "356              31  \n",
       "357              31  \n",
       "\n",
       "[358 rows x 4033 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train/test split\n",
    "np.random.seed(123)\n",
    "_ = torch.manual_seed(123)\n",
    "\n",
    "relevantdata = relevantdata.copy()\n",
    "relevantdata['cancertype_num'] = pd.factorize(relevantdata['cancertype'])[0]\n",
    "relevantdata_train = relevantdata.drop(columns=['cancertype'])\n",
    "relevantdata_train\n",
    "targetdata = targetdata.copy()\n",
    "targetdata['cancertype_num'] = 31\n",
    "\n",
    "np.random.seed(123)\n",
    "_ = torch.manual_seed(123)\n",
    "\n",
    "group_00 = targetdata[(targetdata['event1'] == 0) & (targetdata['event2'] == 0)]  \n",
    "group_01 = targetdata[(targetdata['event1'] == 0) & (targetdata['event2'] == 1)]  \n",
    "group_10 = targetdata[(targetdata['event1'] == 1) & (targetdata['event2'] == 0)]  \n",
    "group_11 = targetdata[(targetdata['event1'] == 1) & (targetdata['event2'] == 1)] \n",
    "\n",
    "train_size_00 = int(0.8 * len(group_00))\n",
    "train_00 = group_00[:train_size_00]\n",
    "test_00 = group_00[train_size_00:]\n",
    "\n",
    "train_size_01 = int(0.8 * len(group_01))\n",
    "train_01 = group_01[:train_size_01]\n",
    "test_01 = group_01[train_size_01:]\n",
    "\n",
    "train_size_10 = int(0.8 * len(group_10))\n",
    "train_10 = group_10[:train_size_10]\n",
    "test_10 = group_10[train_size_10:]\n",
    "\n",
    "train_size_11 = int(0.8 * len(group_11))\n",
    "train_11 = group_11[:train_size_11]\n",
    "test_11 = group_11[train_size_11:]\n",
    "\n",
    "train_targetdata = pd.concat([train_00, train_01, train_10, train_11]).sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "\n",
    "test_targetdata = pd.concat([test_00, test_01, test_10, test_11]).sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "\n",
    "# Calculate the proportion of events in the original data set, training set, and test set\n",
    "total_event1_rate = targetdata['event1'].mean()\n",
    "total_event2_rate = targetdata['event2'].mean()\n",
    "\n",
    "train_event1_rate = train_targetdata['event1'].mean()\n",
    "train_event2_rate = train_targetdata['event2'].mean()\n",
    "\n",
    "test_event1_rate = test_targetdata['event1'].mean()\n",
    "test_event2_rate = test_targetdata['event2'].mean()\n",
    "\n",
    "print(f\"Total event1 rate: {total_event1_rate}, Total event2 rate: {total_event2_rate}\")\n",
    "print(f\"Train event1 rate: {train_event1_rate}, Train event2 rate: {train_event2_rate}\")\n",
    "print(f\"Test event1 rate: {test_event1_rate}, Test event2 rate: {test_event2_rate}\")\n",
    "\n",
    "target_train = train_targetdata.drop(columns=['cancertype'])\n",
    "target_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View all gene names\n",
    "xx = relevantdata_train.drop(columns=['event2','T2','event1','T1','cancertype_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True,  True,  True,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        ...,\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ...,  True,  True,  True]])\n"
     ]
    }
   ],
   "source": [
    "# Sparse connection layer mask matrix\n",
    "mask = torch.load(\"mask.pt\")\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-training\n",
    "np.random.seed(123)\n",
    "_ = torch.manual_seed(123)\n",
    "\n",
    "num_durations = 10\n",
    "num_nodes_shared = (186, 100)\n",
    "num_nodes_indiv = (50)\n",
    "dropout = 0.6\n",
    "lr = 0.001\n",
    "weight_decay = 0.01 \n",
    "batch_size = 100\n",
    "\n",
    "df_train = relevantdata_train\n",
    "\n",
    "# Covariate preprocessing\n",
    "cols_standardize =  xx.columns.tolist()\n",
    "standardize = [([col], StandardScaler()) for col in cols_standardize]\n",
    "x_mapper = DataFrameMapper(standardize)\n",
    "x_train = x_mapper.fit_transform(df_train).astype('float32')\n",
    "\n",
    "labtrans1 = LabTransform(num_durations, scheme='equidistant')\n",
    "get_target1 = lambda df: (df['T1'].values, df['event1'].values)\n",
    "    \n",
    "T1_train = labtrans1.fit_transform(*get_target1(df_train))\n",
    "\n",
    "labtrans2 = LabTransform(num_durations, scheme='equidistant')\n",
    "get_target2 = lambda df: (df['T2'].values, df['event2'].values)\n",
    "\n",
    "T2_train = labtrans2.fit_transform(*get_target2(df_train))\n",
    "\n",
    "T_train = list(T1_train)\n",
    "T1_train_list = list(T1_train)\n",
    "T2_train_list = list(T2_train)\n",
    "cancertype_train_list = list(df_train['cancertype_num'].values)\n",
    "T_train[0] = index_3d_to_1d(T1_train_list[0], T2_train_list[0], cancertype_train_list, 32)\n",
    "T_train[1] = index_2d_to_1d(T1_train_list[1], T2_train_list[1], 2)\n",
    "T_train = tuple(T_train)\n",
    "\n",
    "in_features = x_train.shape[1]\n",
    "num_T1 = num_durations \n",
    "out_features = len(labtrans2.cuts)\n",
    "batch_norm = True\n",
    "\n",
    "net = pretrain_survival(in_features, num_nodes_shared, num_nodes_indiv, num_T1,\n",
    "                       out_features, batch_norm, dropout, mask=mask)\n",
    "\n",
    "optimizer = tt.optim.AdamWR(lr=lr, decoupled_weight_decay=weight_decay)\n",
    "model = multi_task(net, optimizer, alpha=1, duration_index=labtrans1.cuts)\n",
    "epochs = 500\n",
    "callbacks = [tt.callbacks.EarlyStoppingCycle()]\n",
    "verbose = False\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "log = model.fit(x_train, T_train, batch_size, epochs, callbacks, verbose)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "_ = torch.manual_seed(123)\n",
    "\n",
    "target_train['event_combination'] = target_train['event1'].astype(str) + target_train['event2'].astype(str)\n",
    "\n",
    "event_combination_rate_train = target_train['event_combination'].value_counts(normalize=True)\n",
    "\n",
    "n_repeats = 25 \n",
    "n_samples = 20 \n",
    "concordance1 = []\n",
    "concordance2 = []\n",
    "    \n",
    "for i in range(n_repeats):\n",
    "            sampled_data = pd.DataFrame()\n",
    "        \n",
    "            for combination, proportion in event_combination_rate_train.items():\n",
    "                n_combination_samples = int(n_samples * proportion)\n",
    "                combination_data = target_train[target_train['event_combination'] == combination]\n",
    "        \n",
    "                if n_combination_samples > 0:\n",
    "                    sampled_combination_data = combination_data.sample(n=n_combination_samples, random_state=i)\n",
    "                    sampled_data = pd.concat([sampled_data, sampled_combination_data])\n",
    "\n",
    "            if len(sampled_data) < n_samples:\n",
    "                remaining_samples = target_train.drop(sampled_data.index)\n",
    "                extra_samples = remaining_samples.sample(n=n_samples - len(sampled_data), random_state=i)\n",
    "                sampled_data = pd.concat([sampled_data, extra_samples])\n",
    "\n",
    "            lr = 0.001\n",
    "            batch_size = 20\n",
    "\n",
    "            df_train = sampled_data.drop(columns=['event_combination'])\n",
    "            df_test = test_targetdata.drop(columns=['cancertype'])\n",
    " \n",
    "            # Covariate preprocessing\n",
    "            cols_standardize =  xx.columns.tolist()\n",
    "            standardize = [([col], StandardScaler()) for col in cols_standardize]\n",
    "            x_mapper = DataFrameMapper(standardize)\n",
    "            x_train = x_mapper.fit_transform(df_train).astype('float32')\n",
    "            x_test = x_mapper.transform(df_test).astype('float32')\n",
    "\n",
    "            labtrans1 = LabTransform(num_durations, scheme='equidistant')\n",
    "            get_target1 = lambda df: (df['T1'].values, df['event1'].values)\n",
    "    \n",
    "            T1_train = labtrans1.fit_transform(*get_target1(df_train))\n",
    "            T1_test, event1_test = labtrans1.transform(*get_target1(df_test))\n",
    "\n",
    "            labtrans2 = LabTransform(num_durations, scheme='equidistant')\n",
    "            get_target2 = lambda df: (df['T2'].values, df['event2'].values)\n",
    "\n",
    "            T2_train = labtrans2.fit_transform(*get_target2(df_train))\n",
    "            T2_test, event2_test = get_target2(df_test)\n",
    "\n",
    "            T_train = list(T1_train)\n",
    "            T1_train_list = list(T1_train)\n",
    "            T2_train_list = list(T2_train)\n",
    "            cancertype_train_list = list(df_train['cancertype_num'].values)\n",
    "            T_train[0] = index_3d_to_1d(T1_train_list[0], T2_train_list[0], cancertype_train_list, 32)\n",
    "            T_train[1] = index_2d_to_1d(T1_train_list[1], T2_train_list[1], 2)\n",
    "            T_train = tuple(T_train)\n",
    "\n",
    "            in_features = x_train.shape[1]\n",
    "            num_T1 = num_durations \n",
    "            out_features = len(labtrans2.cuts)\n",
    "            batch_norm = True\n",
    "\n",
    "            pre_trained_model = model\n",
    "            # Initialize the new model\n",
    "            new_net = finetune_survival(in_features, num_nodes_shared, num_nodes_indiv, num_T1,\n",
    "                       out_features, batch_norm, dropout, mask=mask)\n",
    "\n",
    "            # Load pre-trained shared layer parameters\n",
    "            new_net.first_layer.load_state_dict(pre_trained_model.net.first_layer.state_dict())\n",
    "            new_net.shared_net.load_state_dict(pre_trained_model.net.shared_net.state_dict())\n",
    "            new_net.risk_nets.load_state_dict(pre_trained_model.net.risk_nets.state_dict())\n",
    "    \n",
    "            new_optimizer = tt.optim.AdamWR(lr=lr, decoupled_weight_decay=weight_decay)\n",
    "            new_model = multi_task(new_net, new_optimizer, alpha=1, duration_index=labtrans1.cuts)\n",
    "            epochs = 500\n",
    "            callbacks = [tt.callbacks.EarlyStoppingCycle()]\n",
    "            verbose = False\n",
    "            new_log = new_model.fit(x_train, T_train, batch_size, epochs, callbacks, verbose)\n",
    "\n",
    "            index = torch.arange(T1_test.size)\n",
    "            cancertype_test=df_test['cancertype_num'].values\n",
    "\n",
    "            x = np.linspace(0, num_durations-1, num_durations)\n",
    "\n",
    "            xnew = np.linspace(0, num_durations-1, 1000)\n",
    "            ynew=[]    \n",
    "            for i in range(T2_test[event1_test==1].size):  \n",
    "                spline_interp = UnivariateSpline(x, (1. - (new_model.predict_pmf_12(x_test)[cancertype_test[event1_test==1],T1_test[event1_test==1],:,index[event1_test==1]].T\n",
    "                                               /new_model.predict_pmf_12(x_test)[cancertype_test[event1_test==1],T1_test[event1_test==1],:,index[event1_test==1]].sum(1)).cumsum(0))[:,i], s=0)\n",
    "                y_spline = spline_interp(xnew)\n",
    "                ynew.append(y_spline)\n",
    "        \n",
    "            ynew_array = np.stack(ynew, axis=1)\n",
    "    \n",
    "            surv = pd.DataFrame(ynew_array, np.linspace(0, labtrans2.cuts.max(), 1000))\n",
    "    \n",
    "            ev = EvalSurv(surv, np.array(T2_test[event1_test==1]), np.array(event2_test)[event1_test==1], censor_surv='km')\n",
    "            time_grid = np.linspace(T2_test[event1_test==1].min(), T2_test[event1_test==1].max(), 100)\n",
    "            concordance1.append(ev.concordance_td())\n",
    "            concordance2.append(ev.integrated_brier_score(time_grid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6393146979260595, 0.7029616724738676, 0.6140667267808837, 0.6519386834986475, 0.7006745362563238, 0.6829679595278246, 0.5871080139372822, 0.7125435540069687, 0.6236933797909407, 0.7343205574912892, 0.6672473867595818, 0.6019163763066202, 0.5566202090592335, 0.7015177065767285, 0.7047038327526133, 0.6114982578397212, 0.5915238954012624, 0.7334494773519163, 0.5725879170423805, 0.6681695220919748, 0.6652613827993255, 0.6358885017421603, 0.6123693379790941, 0.5942290351668169, 0.6677186654643823]\n"
     ]
    }
   ],
   "source": [
    "# C-index for 25 trials\n",
    "print(concordance1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.17206414063102243, 0.18644535354778613, 0.18791078112334406, 0.1708359194701949, 0.19545759708882432, 0.21502874986036014, 0.23174430792174508, 0.186680149982983, 0.23180193692770826, 0.19622272138130967, 0.18009815835956478, 0.1782852622505404, 0.17820039943306437, 0.1954066181951045, 0.24601041518563935, 0.1778422234390337, 0.1720017359886243, 0.19609022847665922, 0.23602510640481983, 0.16979478479856164, 0.21507292669549827, 0.17029463801976233, 0.17400340204490905, 0.17196736711775173, 0.17497449401196022]\n"
     ]
    }
   ],
   "source": [
    "# IBS for 25 trials\n",
    "print(concordance2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
